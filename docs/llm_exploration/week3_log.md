# LLM Exploration Summary

> Only log the conversations that actually moved your thinking forward. You can ask an LLM to help identify which conversations those were. Then ask another LLM to verify that choice. Then maybe verify that verification...

## **Session Focus**  
What sparked this conversation? (1-2 sentences)  
I needed to fine-tune an LLM using **Proximal Policy Optimization (PPO)** to control memory interaction by outputting `<memory_ask>` and `<memory_write>` tokens. This required a structured reward function, supervised pretraining, and a reinforcement learning loop to encourage effective memory management. Additionally, I explored **retrieval-augmented generation (RAG) indexing** for **large-scale PDFs and structured data like Excel files**.

## **Surprising Insights**  

### **Conversation: PPO Fine-Tuning for Memory Tokens**  
**Prompt That Worked:**  
*"How can I fine-tune an LLM with PPO to output `<memory_ask>` and `<memory_write>` tokens for memory control?"*  

**Key Insights:**  
- **Reward shaping is critical**: Initially, I thought simple reward functions would work, but I needed **penalty structures for overuse** of memory calls.  
- **Supervised pretraining first**: PPO fine-tuning **alone is inefficient** without an initial dataset teaching the model what `<memory_ask>` and `<memory_write>` mean.  
- **Reference models stabilize PPO training**: Using a **frozen reference model** improved performance. Without it, PPO collapsed the policy to always generating `<memory_ask>` due to early reward biases.  
- **Fine-tuning GPT-based models with LoRA/QLoRA** was **far more efficient** than full model updates, especially given the cost of running PPO loops on large models.  

---

### **Conversation: RAG for PDFs & Excel Tables**  
**Prompt That Worked:**  
*"How do I get an LLM to interpret structured tables from Excel in a RAG pipeline?"*  

**Key Insights:**  
- **Chunking strategies differ by data type**: **Recursive Character Splitter** works for text-heavy PDFs, but for tables, it **distorts relationships between rows/columns**. Using **structured JSON embeddings** preserves information better.  
- **Multi-query retrieval improved accuracy**: Instead of a single query, **breaking down user questions** into subqueries improved table interpretation.  
- **Hybrid retrieval (ColBERT + BM25) outperformed naive embeddings**: Dense retrieval alone wasnâ€™t enough; **ranking via BM25 improved factual accuracy.**  

---

## **Techniques That Worked**  

- **Fine-tuning with SFT before RLHF**: Pretraining the model on examples of **correct `<memory_ask>` usage before PPO** led to **faster convergence**.  
- **Multi-query retrieval**: Asking multiple slightly different queries improved factual recall in **table-based RAG**.  
- **ColBERT + BM25 hybrid retrieval**: This outperformed naive embedding-based search for structured PDFs/Excel files.  

---

## **Dead Ends Worth Noting**  

- **Naively increasing PPO reward strength collapsed training**: If I over-rewarded `<memory_ask>`, the model **started using it excessively**. Had to introduce **penalties for unnecessary calls**.  
- **Early RAG attempts failed due to naive chunking**: Splitting **Excel tables using standard text chunkers** lost relationships between rows/columns. Required a **graph-based approach** to retain structure.  

---

## **Next Steps**  

- [ ] **Test token-efficient alternatives to PPO** (e.g., **DPO**) to reduce fine-tuning costs.  
- [ ] **Refine table-based RAG retrieval** with **graph-based embeddings** instead of naive chunking.  
- [ ] **Benchmark different fine-tuning methods (QLoRA vs. full finetuning) for PPO training stability.**  

---

### **Verification Statement**  
This summary was **generated by an LLM, and validated against past conversations**. I used LLMs to:  
- Identify the most **impactful** conversations.  
- **Verify insights** for surprise factor.  
- **Assess next-step feasibility** based on prior failed attempts.  

